# 文献購読: Deal or No Deal? End-to-End Learning for Negotiation Dialogues
2018/02/19

## お断り
誤りがあるかもしれません。その場合は下記のコメントフォームもしくはTwitterアカウントへのReplyにてお知らせ下さい。

## TL;DR

[元論文: Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://arxiv.org/abs/1706.05125)

目的は自然言語によって２者間の自動交渉を行なうこと。
2人でいくつかの本、ボール、帽子を山分けする。
それぞれのアイテムに割り当てられた効用(報酬, スコア)はお互い異なる。
提案手法は以下の4つのGRUによって構成される。

* 入力（自分の所持する商品とその効用）をエンコードする$GRU_g$
* 対話を理解し発話を生成する$GRU_w$
* 対話をエンコードして出力(相手に渡す商品とその数)をデコードする双方向GRU $GRU_o$

更に、強化学習を自分同士を対戦させることで行なう。
また、発話候補を複数生成し、シミュレーションによってその結果得られる効用を推定し、よりよい発話候補を選択する*dialogue rollout*を行なう。
これによって対botの交渉能力は全体として上昇したが、対人間の交渉能力の上昇は部分的だった。

## 背景

自動交渉をするためには以下を実現する必要がある。

* 自然言語処理
人間との自然言語でのやりとり
* 推論
相手の意図の定式化

提案手法ではこれらを解決する。

## 目的

自然言語を用いた対話によって自動交渉を行う。


本稿における交渉手順は以下の通り。

1. お互いが場に存在する各アイテムの数、そして各アイテムの自分自身にとっての効用値を知る。
![](https://i.imgur.com/L79pons.png)
2. 交渉相手と対話する。
![](https://i.imgur.com/hkMn41L.png)
3. 対話結果から自分が場から取るアイテムの数を出力する。
![](https://i.imgur.com/LOa53qW.png)
4. 双方の提案が噛み合えば交渉が成立する。

交渉条件は次の通り。

* 本・帽子・ボールを取引する
* 全ての商品は非負の価値を持つ
* 全ての商品は1人以上にとって正の価値を持つ
* 最初に自分が所持する商品の自分にとっての価値の合計は10
* 双方が10回発話しても合意に至らない場合交渉を打ち切る事ができる
* 双方の提案が衝突したら報酬は0

## データセット

Mechanical Turk上の人間同士に2236件のシナリオから5808件の交渉をさせデータセットを作成した。今回は次のような雇用条件を設定した。

* 雇用側からのレビュー評価: 95%
* こなした仕事の数: 5000件

彼らの1交渉あたりの給料は$0.15だが、その交渉における最も高い得点を出した場合はボーナスとして更に$0.05が与えられる。

## 提案手法
### ネットワーク構成
1. $GRU_g$で入力$g$をエンコードする
最終隠れ層を$h^g$と呼ぶ
![](https://i.imgur.com/9wxRXmv.png)
2. $GRU_w$で以下を入力として発話を生成する
* $GRU_w$の直前の隠れ層$h_{t-1}^{w}$
* 直前に発話された単語$x_{t-1}$
* $GRU_g$の最終隠れ層 $h^g$
![](https://i.imgur.com/Vu7SpAU.png)
3. 相手か自分が「<交換>」を宣言したら
$GRU_o$で以下を入力として出力を生成する
* $GRU_o$の直前の隠れ層$h_{t-1}^o$
* $x_t$と$h_t^w$
![](https://i.imgur.com/ryJ9bHh.png)

### 上記ネットワークの利点と欠点、その改善方法
上記のネットワーク構成による交渉では、次のような利点と欠点があることがわかった。

利点
* 人間にとても良く似た発話・交渉ができる

欠点
* 自分にとって最適な合意から離れた合意をしてしまう

そこで、強化学習とdialogue rolloutによる交渉能力の強化を行なうこと担った。

### 強化学習
自分同士を対戦させて強化学習を行なう。ただし、片方はモデルを固定し学習させない。

$x_t \in X^A$に対する報酬を次のように定義する。

$R(x_t) = \Sigma_{x_t \in X^A}\gamma^{T-t}(r^A(o)-\mu)$

なお、変数はそれぞれ以下の通りとする。
* $X^A$
エージェントAが生成した単語列
* $\gamma$
割引率
* $T$
対話に置ける単語数
* $r^A$
対話完了時にエージェント$A$が得る得点

### Dialogue Rollout
以下のようにシミュレーションを行い最も得点が高い応答を選ぶ。
![](https://i.imgur.com/aKzbegr.png)


## 実験結果
対提案手法(DR無し, 強化学習無し)
<table>
    <thead>
        <tr>
            <th></th>
            <th>スコア(全体)</th>
            <th>スコア(合意成功時)</th>
            <th>合意形成率(%)</th>
            <th>パレート最適率(%)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>DR無し<br>強化学習無し</th>
            <td>5.4 vs 5.5</td>
            <td>6.2 vs 6.2</td>
            <td>87.9</td>
            <td>49.6</td>
        </tr>
        <tr>
            <th>DR有り<br>強化学習無し</th>
            <td>7.3 vs 5.1</td>
            <td>7.9 vs 5.5</td>
            <td>92.9</td>
            <td>63.7</td>
        </tr>
        <tr>
            <th>DR無し<br>強化学習有り</th>
            <td>7.1 vs 4.2</td>
            <td>7.9 vs 4.7</td>
            <td>89.9</td>
            <td>58.6</td>
        </tr>
        <tr>
            <th>DR有り<br>強化学習有り</th>
            <td>8.3 vs 4.2</td>
            <td>8.8 vs 4.5</td>
            <td>94.4</td>
            <td>74.8</td>
        </tr>
    </tbody>
</table>

対人間
<table>
    <thead>
        <tr>
            <th></th>
            <th>スコア(全体)</th>
            <th>スコア(合意成功時)</th>
            <th>合意形成率(%)</th>
            <th>パレート最適率(%)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>DR無し<br>強化学習無し</th>
            <td>4.7 vs 5.8</td>
            <td>6.2 vs 7.6</td>
            <td>76.5</td>
            <td>66.2</td>
        </tr>
        <tr>
            <th>DR有り<br>強化学習無し</th>
            <td>5.2 vs 5.4</td>
            <td>7.1 vs 7.4</td>
            <td>72.1</td>
            <td>78.3</td>
        </tr>
        <tr>
            <th>DR無し<br>強化学習有り</th>
            <td>4.3 vs 5.0</td>
            <td>6.4 vs 7.5</td>
            <td>67.3</td>
            <td>69.1</td>
        </tr>
        <tr>
            <th>DR有り<br>強化学習有り</th>
            <td>4.6 vs 4.2</td>
            <td>8.0 vs 7.1</td>
            <td>57.2</td>
            <td>82.4</td>
        </tr>
    </tbody>
</table>

## 考察

何も強化を加えていない提案手法より、強化学習+DRによって強化した提案手法の方が交渉時間が長いことが分かった。筆者らはこれについて次のように考察している。

1. 強化学習+DRはアグレッシブな交渉を仕掛けてくる
2. アグレッシブな交渉は交渉時間を長引かせる
3. 人間は長時間交渉をされると例え報酬が0になるとしても交渉を諦めてしまう
4. 双方のスコアが下がる

また、生成された発話は圧倒的に多くがとても流暢だった。
生成された発話のうち76%はそのまま教師データに存在した。

また、筆者らは「提案手法は欺くことを覚えた」と述べている。

## まとめ

* 提案手法は自然言語を用いた人間との交渉にチャレンジした
* 強化学習及びDialogue Rolloutによって対botのスコアは上昇した
* 提案手法による交渉がアグレッシブなので交渉相手を疲弊させることから対人間のスコアは低下した
* GRUとノイズの少ないデータセットによって流暢な発話を行なうことが出来た